<div align="center">
  
  <img src="https://github.com/user-attachments/assets/fb9a7a78-57fc-493a-8961-e91c1786e6c5" alt="Chat with MiniCPM-V 2.6, a GPT-4V Level MLLM" width="100" height="100">
  <h1>Chat with MiniCPM-V 2.6, a GPT-4V Level MLLM</h1>
  <img src="https://github.com/user-attachments/assets/ddb1acf9-5dd3-44b3-b1a4-96d5fb552d95" alt="Chat with MiniCPM-V 2.6, a GPT-4V Level MLLM" width="640" height="360">
  <br/>
</div>

## Overview

**MiniCPM-V 2.6** is a GPT-4V-level multi-modal language model (MLLM) designed to handle single image, multi-image, and video inputs on your mobile device. Powered by [**MiniCPM-V 2.6**](https://huggingface.co/openbmb/MiniCPM-V-2_6), this is the most advanced model in the MiniCPM-V series, delivering cutting-edge performance and versatility for your applications.

## Getting Started

Follow these steps to set up and run the project:

### 1. Install Dependencies

Ensure all necessary packages are installed by running:

```bash
pip install -r requirements.txt
```

### 2. Start the API Server

Launch the API server powered by [LitServe](https://github.com/Lightning-AI/LitServe):

```bash
python server.py
```

### 3. Launch the Streamlit App

Start the Streamlit application with the following command:

```bash
streamlit run app.py
```

## About

This project is developed and maintained with ❤️ by [Bhimraj Yadav](https://github.com/bhimrazy).
